name: spark-session-utils CI

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'spark-session-utils/**'
      - '.github/workflows/spark-session-utils-ci.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'spark-session-utils/**'
      - '.github/workflows/spark-session-utils-ci.yml'

jobs:
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11']

    steps:
    - uses: actions/checkout@v6

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install Java (required for PySpark)
      uses: actions/setup-java@v3
      with:
        distribution: 'temurin'
        java-version: '11'

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('spark-session-utils/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      working-directory: spark-session-utils
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Lint with ruff
      working-directory: spark-session-utils
      run: |
        ruff check src tests

    - name: Check formatting with black
      working-directory: spark-session-utils
      run: |
        black --check src tests

    - name: Type check with mypy
      working-directory: spark-session-utils
      run: |
        mypy src
      continue-on-error: true  # Don't fail on type errors yet

    - name: Run unit tests
      working-directory: spark-session-utils
      run: |
        pytest tests/unit -v --cov=spark_session_utils --cov-report=xml --cov-report=term-missing

    - name: Run integration tests
      working-directory: spark-session-utils
      run: |
        pytest tests/integration -v -m integration
      continue-on-error: true  # Integration tests may be flaky

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: spark-session-utils/coverage.xml
        flags: spark-session-utils
        name: spark-session-utils-${{ matrix.python-version }}

  build:
    name: Build package
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v6

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.10'

    - name: Install build tools
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      working-directory: spark-session-utils
      run: |
        python -m build

    - name: Check package
      working-directory: spark-session-utils
      run: |
        twine check dist/*

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: spark-session-utils-dist
        path: spark-session-utils/dist/
