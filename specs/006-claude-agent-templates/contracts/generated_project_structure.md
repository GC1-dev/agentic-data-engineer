# Generated Project Structure Contract

**Version**: 1.0.0
**Date**: 2025-11-23
**Feature**: Claude Agent Template System

## Overview

This contract defines the exact structure, file naming conventions, and content requirements for projects generated by the Claude agent. This is a **strict contract** - all generated projects MUST conform to this specification for validation to pass (FR-018).

---

## Directory Structure Contract (FR-003)

### 9-Directory Standard (Non-Negotiable)

**Contract**: Every generated project MUST contain exactly these 9 top-level directories:

```
{project_name}/
├── src/                    # Source code and business logic
├── pipelines/              # Data pipeline implementations
├── dashboards/             # Dashboard definitions and queries
├── databricks_apps/        # Databricks Apps code
├── monte_carlo/            # Monte Carlo monitoring configs (always present, may be empty)
├── data_validation/        # Data quality validation rules (always present, may be empty)
├── tests/                  # Test suites
├── config/                 # Configuration files
├── docs/                   # Documentation
└── databricks/             # Databricks Asset Bundle configuration
```

**Validation Rule**: No additional top-level directories allowed. Exactly 9, no more, no less.

---

## Required Files Contract

### Root Level Files

**Contract**: These files MUST exist at project root:

| File | Required | Purpose | Contract Spec |
|------|----------|---------|---------------|
| `README.md` | ✅ Yes | Project documentation | See README Content Contract below |
| `requirements.txt` | ✅ Yes | Python dependencies | See Dependencies Contract below |
| `.gitignore` | ✅ Yes | Git exclusions | See .gitignore Contract below |
| `pyproject.toml` | ✅ Yes | Python project metadata | See pyproject.toml Contract below |
| `.env.example` | ✅ Yes | Environment variable template | Contains placeholders, no secrets |

---

## src/ Directory Contract

**Purpose**: Application source code and business logic

**Structure Contract**:
```
src/
├── __init__.py                  # Package marker (required)
├── models/                      # Data models
│   └── __init__.py
├── transformations/             # Data transformation logic
│   └── __init__.py
└── utils/                       # Utility functions
    └── __init__.py
```

**Validation Rules**:
- `__init__.py` required in src/ and all subdirectories
- Minimum structure: models/, transformations/, utils/
- Additional subdirectories allowed based on requirements

---

## pipelines/ Directory Contract

**Purpose**: Pipeline implementations (batch, streaming, or both)

### Batch Pipeline Structure
```
pipelines/
├── __init__.py
├── bronze/
│   ├── __init__.py
│   └── ingest_{source}.py       # One file per data source
├── silver/
│   ├── __init__.py
│   └── transform_{entity}.py    # One file per entity
└── gold/
    ├── __init__.py
    └── aggregate_{metric}.py    # One file per metric
```

### Streaming Pipeline Structure (if streaming feature enabled)
```
pipelines/
├── streaming/
│   ├── __init__.py
│   ├── stream_processor.py      # Main streaming logic
│   └── checkpoint_manager.py    # Checkpoint management
└── [bronze/silver/gold as above]
```

**Validation Rules**:
- If `transformation_layers` includes "bronze", bronze/ directory must exist
- If `transformation_layers` includes "silver", silver/ directory must exist
- If `transformation_layers` includes "gold", gold/ directory must exist
- If streaming feature enabled, streaming/ directory must exist

---

## dashboards/ Directory Contract

**Purpose**: Dashboard definitions, SQL queries, and visualization configs

**Structure Contract**:
```
dashboards/
├── README.md                    # Dashboard documentation
├── queries/                     # SQL queries
│   └── example_query.sql
└── definitions/                 # Dashboard JSON definitions
    └── example_dashboard.json
```

**Content Contract**:
- `README.md` must explain how to deploy dashboards
- Queries should be parameterized using Jinja2 syntax
- Dashboard definitions should be valid Databricks Dashboard JSON

---

## databricks_apps/ Directory Contract

**Purpose**: Databricks Apps code (if applicable)

**Structure Contract**:
```
databricks_apps/
├── README.md                    # App documentation
└── app.py                       # Main app entry point (if apps requested)
```

**Conditional**: Directory always present but may contain only README.md if no apps requested

---

## monte_carlo/ Directory Contract (FR-010)

**Purpose**: Monte Carlo observability configuration

**Structure Contract (if Monte Carlo feature enabled)**:
```
monte_carlo/
├── __init__.py
├── monitors.py                  # Monitor definitions
└── README.md                    # Monte Carlo setup instructions
```

**Structure Contract (if Monte Carlo feature NOT enabled)**:
```
monte_carlo/
└── README.md                    # Placeholder explaining how to add Monte Carlo later
```

**Validation Rules**:
- Directory always present (part of 9-directory standard)
- If `monte_carlo` in `optional_features`, must contain monitors.py
- If NOT in `optional_features`, contains only README.md

---

## data_validation/ Directory Contract (FR-010)

**Purpose**: Data quality validation rules and expectations

**Structure Contract (if data validation feature enabled)**:
```
data_validation/
├── __init__.py
├── expectations/                # Great Expectations suite (if using GE)
│   └── example_suite.json
├── rules.py                     # Custom validation rules
└── README.md                    # Validation documentation
```

**Structure Contract (if data validation feature NOT enabled)**:
```
data_validation/
└── README.md                    # Placeholder explaining how to add validation later
```

**Validation Rules**:
- Directory always present (part of 9-directory standard)
- If `data_validation` in `optional_features`, must contain rules.py
- If NOT in `optional_features`, contains only README.md

---

## tests/ Directory Contract (FR-010)

**Purpose**: Test suites for code validation

**Structure Contract (if testing feature enabled)**:
```
tests/
├── __init__.py
├── conftest.py                  # Pytest configuration and fixtures
├── unit/                        # Unit tests
│   ├── __init__.py
│   ├── test_models.py
│   └── test_transformations.py
├── integration/                 # Integration tests
│   ├── __init__.py
│   └── test_pipelines.py
└── fixtures/                    # Test data fixtures
    └── sample_data.json
```

**Structure Contract (if testing feature NOT enabled)**:
```
tests/
├── __init__.py
└── README.md                    # Placeholder explaining how to add tests
```

**Validation Rules**:
- Directory always present
- If `testing` in `optional_features`:
  - Must contain conftest.py
  - Must have unit/ and integration/ subdirectories
  - pytest.ini must exist at project root
- If NOT in `optional_features`, minimal structure with README.md

---

## config/ Directory Contract (FR-004)

**Purpose**: Environment-specific configuration files

**Structure Contract** (STRICT):
```
config/
├── project.yaml                 # Project metadata (required)
├── local.yaml                   # Local development config (required)
├── lab.yaml                     # Lab environment config (if lab in environments)
├── dev.yaml                     # Dev environment config (required)
└── prod.yaml                    # Production environment config (required)
```

**project.yaml Schema Contract**:
```yaml
name: {project_name}
version: "1.0.0"
description: {project_description}
python_version: "{python_version}"
databricks_runtime: "{databricks_runtime}"
```

**Environment Config Schema Contract** (local/lab/dev/prod.yaml):
```yaml
environment: {environment_name}  # Must match filename

# Databricks settings
workspace:
  url: ${WORKSPACE_URL}          # Environment variable reference

# Unity Catalog settings
unity_catalog:
  catalog: ${CATALOG_NAME}       # Environment variable reference
  schema: ${SCHEMA_NAME}         # Environment variable reference

# Compute settings
compute:
  size: {small|medium|large}
  autoscaling:
    enabled: {true|false}
    min_workers: {int}
    max_workers: {int}
  spot_instances: {true|false}

# Storage paths
storage:
  checkpoint_location: /dbfs/checkpoints/${PROJECT_NAME}
  data_location: /dbfs/data/${PROJECT_NAME}
```

**Validation Rules**:
- All YAML files must have valid syntax (no parsing errors)
- Environment variable references use `${VAR_NAME}` syntax
- No hardcoded credentials allowed
- `environment` field must match filename (dev.yaml → environment: dev)

---

## docs/ Directory Contract (FR-017)

**Purpose**: Project documentation

**Structure Contract**:
```
docs/
├── architecture.md              # Architecture diagrams and design
├── deployment.md                # Deployment instructions
├── api_reference.md             # API documentation (if custom modules)
└── troubleshooting.md           # Common issues and solutions
```

**Content Requirements**:
- `architecture.md` MUST contain Mermaid diagram showing data flow
- `deployment.md` MUST contain step-by-step Databricks deployment instructions
- `api_reference.md` required only if custom src/ modules exist
- All docs must reference project-specific names/paths, not generic templates

---

## databricks/ Directory Contract (FR-009)

**Purpose**: Databricks Asset Bundle configuration

**Structure Contract** (STRICT):
```
databricks/
├── bundle.yml                   # Main Asset Bundle config (required)
└── resources/                   # Resource definitions
    ├── jobs.yml                 # Job definitions
    └── pipelines.yml            # DLT pipeline definitions (if applicable)
```

**bundle.yml Schema Contract**:
```yaml
bundle:
  name: {project_name}

include:
  - resources/*.yml

targets:
  local:
    mode: development
    workspace:
      host: ${WORKSPACE_URL}

  dev:
    mode: development
    workspace:
      host: ${WORKSPACE_URL}

  prod:
    mode: production
    workspace:
      host: ${WORKSPACE_URL}
```

**Validation Rules**:
- Must conform to Databricks Asset Bundle specification v1
- Must include targets for each environment in `environment_configs`
- `bundle.name` must be filesystem-safe (matches project_name validation)
- All resource files must have valid YAML syntax

---

## README.md Content Contract (FR-017)

**Required Sections** (in order):

### 1. Project Overview
```markdown
# {Project Name}

{Project description from requirements}

**Created**: {timestamp}
**Python Version**: {python_version}
**Databricks Runtime**: {databricks_runtime}
```

### 2. Features
```markdown
## Features

- ✅ {Transformation layers: bronze/silver/gold}
- ✅ {Feature 1: e.g., Streaming pipeline support}
- ✅ {Feature 2: e.g., Monte Carlo observability}
- ✅ {CI/CD: platform name}
```

### 3. Project Structure
```markdown
## Project Structure

\`\`\`
{Full directory tree with inline comments}
\`\`\`
```

### 4. Setup Instructions
```markdown
## Setup

1. Clone repository
2. Install dependencies: \`pip install -r requirements.txt\`
3. Configure environment variables: \`cp .env.example .env\`
4. Edit \`.env\` with your workspace URL and credentials
5. Run tests: \`pytest tests/\`
```

### 5. Architecture Diagram (Mermaid)
```markdown
## Architecture

\`\`\`mermaid
graph LR
    A[Data Sources] --> B[Bronze Layer]
    B --> C[Silver Layer]
    C --> D[Gold Layer]
    D --> E[Dashboards]
\`\`\`
```

### 6. Deployment
```markdown
## Deployment

### Local Development
\`\`\`bash
databricks bundle deploy -t local
\`\`\`

### Production
\`\`\`bash
databricks bundle deploy -t prod
\`\`\`
```

### 7. API Reference (if custom modules)
```markdown
## API Reference

### Module: src.transformations

#### Function: transform_data()
...
```

### 8. Security Reminders
```markdown
## Security

⚠️ **Never commit secrets to version control**

- Store credentials in environment variables
- Use Databricks secrets for production
- Review .gitignore to ensure sensitive files excluded
```

**Validation Rules**:
- All 8 sections must be present
- Mermaid diagram must be syntactically valid
- All code blocks must specify language
- All {placeholders} must be replaced with actual values

---

## requirements.txt Contract (FR-008)

**Format**: Pinned versions with comments

**Example Contract**:
```txt
# Core dependencies
pyspark==3.5.0              # Spark DataFrame operations
delta-spark==3.0.0          # Delta Lake integration
databricks-sdk==0.18.0      # Databricks API client

# Optional: Streaming (if streaming feature enabled)
kafka-python==2.0.2         # Kafka integration

# Optional: Monte Carlo (if monte_carlo feature enabled)
pycarlo==0.50.0             # Monte Carlo observability

# Optional: Data Validation (if data_validation feature enabled)
great-expectations==0.18.0  # Data quality validation
pydantic==2.5.0             # Data validation models

# Optional: Testing (if testing feature enabled)
pytest==7.4.0               # Test framework
pytest-cov==4.1.0           # Coverage reporting
chispa==0.9.2               # PySpark testing utilities

# Development tools
black==23.12.0              # Code formatting
ruff==0.1.8                 # Linting
mypy==1.7.0                 # Type checking
```

**Validation Rules**:
- All versions must be pinned (==X.Y.Z, not >=X.Y.Z)
- All packages must be compatible with specified Python version
- All packages must be compatible with specified Databricks Runtime
- Each dependency must have inline comment explaining purpose
- Optional features only included if enabled

---

## .gitignore Contract

**Required Exclusions**:
```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
ENV/

# Databricks
.databricks/
*.dbtoken

# Secrets (SECURITY REQUIREMENT)
.env
*.key
*.pem
credentials.json
secrets/

# IDE
.vscode/
.idea/
*.swp

# Testing
.pytest_cache/
.coverage
htmlcov/

# OS
.DS_Store
Thumbs.db

# Data (large files)
*.parquet
*.delta
data/
checkpoints/
```

**Validation Rule**: All secret file patterns from Security Considerations must be present

---

## pyproject.toml Contract

**Minimum Required Content**:
```toml
[project]
name = "{project_name}"
version = "1.0.0"
description = "{project_description}"
requires-python = ">={python_version}"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = ["-v", "--strict-markers"]

[tool.black]
line-length = 100
target-version = ["py{python_major}{python_minor}"]

[tool.ruff]
line-length = 100
target-version = "py{python_major}{python_minor}"
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "{python_version}"
warn_return_any = true
disallow_untyped_defs = false
```

---

## CI/CD Workflow Contracts

### GitHub Actions Contract (if ci_cd_platform == "github_actions")

**File**: `.github/workflows/ci.yml`

**Required Jobs**:
```yaml
name: CI

on:
  pull_request:
  push:
    branches: [main, dev]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "{python_version}"
      - run: pip install black ruff mypy
      - run: black --check .
      - run: ruff check .
      - run: mypy src/

  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "{python_version}"
      - run: pip install -r requirements.txt
      - run: pytest tests/

  validate-bundle:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: databricks bundle validate
```

### GitLab CI Contract (if ci_cd_platform == "gitlab_ci")

**File**: `.gitlab-ci.yml`

**Required Stages**:
```yaml
stages:
  - lint
  - test
  - validate

lint:
  stage: lint
  image: python:{python_version}
  script:
    - pip install black ruff mypy
    - black --check .
    - ruff check .
    - mypy src/

test:
  stage: test
  image: python:{python_version}
  script:
    - pip install -r requirements.txt
    - pytest tests/

validate-bundle:
  stage: validate
  script:
    - databricks bundle validate
```

### Azure DevOps Contract (if ci_cd_platform == "azure_devops")

**File**: `azure-pipelines.yml`

**Required Pipeline**:
```yaml
trigger:
  - main
  - dev

pool:
  vmImage: 'ubuntu-latest'

stages:
- stage: Lint
  jobs:
  - job: PythonLint
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '{python_version}'
    - script: |
        pip install black ruff mypy
        black --check .
        ruff check .
        mypy src/

- stage: Test
  jobs:
  - job: PythonTest
    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '{python_version}'
    - script: |
        pip install -r requirements.txt
        pytest tests/

- stage: Validate
  jobs:
  - job: BundleValidate
    steps:
    - script: databricks bundle validate
```

---

## Validation Checklist

Generated projects are validated against this contract using automated validators:

### StructureValidator Checks
- [ ] Exactly 9 top-level directories present
- [ ] All required files exist at root level
- [ ] All required subdirectory structures present
- [ ] No unexpected directories at root level

### ConfigValidator Checks
- [ ] All YAML files have valid syntax
- [ ] Environment names match filenames
- [ ] No hardcoded credentials detected
- [ ] All required config fields present

### BundleValidator Checks
- [ ] bundle.yml conforms to Databricks Asset Bundle spec
- [ ] All targets defined for specified environments
- [ ] Resource files have valid YAML syntax

### CompatibilityChecker Checks
- [ ] Python version compatible with Databricks Runtime
- [ ] All dependencies compatible with Python version
- [ ] All dependencies compatible with Databricks Runtime
- [ ] No conflicting package versions

### README Validator Checks
- [ ] All 8 required sections present
- [ ] Mermaid diagram syntactically valid
- [ ] All placeholders replaced
- [ ] Code blocks have language specified

---

**Contract Status**: Draft v1.0.0
**Enforcement**: All contracts enforced by validation pipeline before generation completion
**Changes**: Any changes to this contract require corresponding updates to:
1. Template files
2. Validation code
3. Test fixtures
4. Documentation
